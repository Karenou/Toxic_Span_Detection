{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7aaffd5",
   "metadata": {},
   "source": [
    "## Required package\n",
    "- spacy==2.2.4\n",
    "    - pip install spacy==2.2.4\n",
    "- download en_core_web_sm\n",
    "    - python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf138faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "import os\n",
    "import string\n",
    "import itertools\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae766546",
   "metadata": {},
   "source": [
    "- util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "771d9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_CHARACTERS = string.whitespace\n",
    "\n",
    "def _contiguous_ranges(span_list):\n",
    "    \"\"\"\n",
    "    Group character-level labels into range of intervals: [1, 2, 3, 5, 6, 7] -> [(1,3), (5,7)].\n",
    "    Returns begin and end inclusive\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for _, span in itertools.groupby(enumerate(span_list), lambda p: p[1] - p[0]):\n",
    "        span = list(span)\n",
    "        output.append((span[0][1], span[-1][1]))\n",
    "    return output\n",
    "\n",
    "\n",
    "def _fix_spans(spans, text, special_characters=SPECIAL_CHARACTERS, collapse=False):\n",
    "    \"\"\"\n",
    "    Applies minor edits to trim spans and remove singletons.\n",
    "    If spans begin/end in the middle of a word, correct according to collapse strategy:\n",
    "        If false, expand spans until word limits; if true collapse until word limits\n",
    "    \"\"\"\n",
    "    cleaned_spans = []\n",
    "    \n",
    "    for begin, end in _contiguous_ranges(spans):\n",
    "        # Remove special characters\n",
    "        while text[begin] in special_characters and begin < end:\n",
    "            begin += 1\n",
    "        while text[end] in special_characters and begin < end:\n",
    "            end -= 1\n",
    "            \n",
    "        # Keep full word\n",
    "        while 0 < begin < end and text[begin - 1].isalnum():\n",
    "            offset_move = 1 if collapse else -1\n",
    "            begin += offset_move\n",
    "        while len(text) - 1 > end > begin and text[end + 1].isalnum():\n",
    "            offset_move = -1 if collapse else 1\n",
    "            end += offset_move\n",
    "            \n",
    "        # Remove singletons (only one character)\n",
    "        if end - begin > 1:\n",
    "            cleaned_spans.extend(range(begin, end + 1))\n",
    "            \n",
    "    return cleaned_spans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7ee8d",
   "metadata": {},
   "source": [
    "- dataset class, can be converted to torch.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82dc1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicDataset():\n",
    "    def __init__(self, data_path, split):\n",
    "        \"\"\"\n",
    "        @param data_path: base_path of the data folder\n",
    "        @param split: name of the csv file without .csv suffix\n",
    "        \"\"\"\n",
    "        self.tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        self.original_sentences, self.original_spans, self.fixed_spans = \\\n",
    "            self.get_sentences_from_data_split(data_path, split)\n",
    "\n",
    "        self.tokens, self.offsets, self.labels_ids = \\\n",
    "            self.preprocess_and_tokenize(self.original_sentences, self.fixed_spans)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokens = self.tokens[index]\n",
    "        offsets = self.offsets[index]\n",
    "        label_ids = self.labels_ids[index]\n",
    "        original_spans = self.original_spans[index]\n",
    "\n",
    "        return tokens, label_ids, offsets, original_spans\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sentences_from_data_split(data_path, split):\n",
    "        \"\"\"\n",
    "        @param data_path: base_path of the data folder\n",
    "        @param split: name of the csv file without .csv suffix\n",
    "        \"\"\"\n",
    "        sentences = []\n",
    "        original_spans = []\n",
    "        fixed_spans = []\n",
    "        data = pd.read_csv(os.path.join(data_path, split + '.csv'))\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            if split == 'tsd_test':\n",
    "                span = fixed_span = []\n",
    "            else:\n",
    "                span = literal_eval(data['spans'][i])\n",
    "                fixed_span = _fix_spans(span, data['text'][i])\n",
    "            sentences.append(data['text'][i])\n",
    "            original_spans.append(span)\n",
    "            fixed_spans.append(fixed_span)\n",
    "\n",
    "        return sentences, original_spans, fixed_spans\n",
    "\n",
    "    def preprocess_and_tokenize(self, sentences, spans):\n",
    "        \"\"\"\n",
    "        @param sentences: a list of posts, List[str]\n",
    "        @param spans: a list of toxic spans, List[List[int]]\n",
    "        \"\"\"\n",
    "        all_tokens = []\n",
    "        all_offsets = []\n",
    "        all_label_ids = []\n",
    "\n",
    "        for sentence, span in zip(sentences, spans):\n",
    "            tokenized = self.tokenizer(sentence)\n",
    "            tokens = [token.text for token in tokenized]\n",
    "            token_offset = [(token.idx, token.idx + len(token.text)) for token in tokenized]\n",
    "\n",
    "            all_tokens.append(tokens)\n",
    "            all_offsets.append(token_offset)\n",
    "            all_label_ids.append([self.off2tox(offset, span) for offset in token_offset])\n",
    "\n",
    "        return all_tokens, all_offsets, all_label_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def off2tox(offsets, spans):\n",
    "        \"\"\"\n",
    "        @param offsets: a tuple indicates the start and end position of the token in the sentence\n",
    "        @param spans: toxic span label, List[int]\n",
    "        \"\"\"\n",
    "        # Padded items\n",
    "        if offsets == (0, 0):\n",
    "            return 0\n",
    "        toxicity = offsets[0] in spans\n",
    "        return int(toxicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b018e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "839c26d6",
   "metadata": {},
   "source": [
    "- use the tokens and label_ids for each post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71ce2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ToxicDataset(\"data\", \"tsd_trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1246abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, label_ids, offsets, original_spans = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2bce211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Because', 'he', \"'s\", 'a', 'moron', 'and', 'a', 'bigot', '.', 'It', \"'s\", 'not', 'any', 'more', 'complicated', 'than', 'that', '.']\n",
      "[0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "# print(offsets)\n",
    "print(label_ids)\n",
    "# print(original_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de58f833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.6.0",
   "language": "python",
   "name": "data_mining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
