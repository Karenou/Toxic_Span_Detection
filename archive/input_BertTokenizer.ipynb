{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba3ea04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import string\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c4ba92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560506c2",
   "metadata": {},
   "source": [
    "- conda install -c conda-forge transformers\n",
    "- conda install -c conda-forge huggingface_hub==0.2.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8aafa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b6e913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_CHARACTERS = string.whitespace\n",
    "\n",
    "def _contiguous_ranges(span_list):\n",
    "    \"\"\"\n",
    "    Group character-level labels into range of intervals: [1, 2, 3, 5, 6, 7] -> [(1,3), (5,7)].\n",
    "    Returns begin and end inclusive\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    for _, span in itertools.groupby(enumerate(span_list), lambda p: p[1] - p[0]):\n",
    "        span = list(span)\n",
    "        output.append((span[0][1], span[-1][1]))\n",
    "    return output\n",
    "\n",
    "\n",
    "def _fix_spans(spans, text, special_characters=SPECIAL_CHARACTERS, collapse=False):\n",
    "    \"\"\"\n",
    "    Applies minor edits to trim spans and remove singletons.\n",
    "    If spans begin/end in the middle of a word, correct according to collapse strategy:\n",
    "        If false, expand spans until word limits; if true collapse until word limits\n",
    "    \"\"\"\n",
    "    cleaned_spans = []\n",
    "    \n",
    "    for begin, end in _contiguous_ranges(spans):\n",
    "        # Remove special characters\n",
    "        while text[begin] in special_characters and begin < end:\n",
    "            begin += 1\n",
    "        while text[end] in special_characters and begin < end:\n",
    "            end -= 1\n",
    "            \n",
    "        # Keep full word\n",
    "        while 0 < begin < end and text[begin - 1].isalnum():\n",
    "            offset_move = 1 if collapse else -1\n",
    "            begin += offset_move\n",
    "        while len(text) - 1 > end > begin and text[end + 1].isalnum():\n",
    "            offset_move = -1 if collapse else 1\n",
    "            end += offset_move\n",
    "            \n",
    "        # Remove singletons (only one character)\n",
    "        if end - begin > 1:\n",
    "            cleaned_spans.extend(range(begin, end + 1))\n",
    "            \n",
    "    return cleaned_spans\n",
    "\n",
    "def get_sentences_from_data_split(data_path, split):\n",
    "    \"\"\"\n",
    "    @param data_path: base path to load data\n",
    "    @param split: tsd_train or tsd_trial or tsd_test\n",
    "    return sentences (List[str]), original_spans (List[List[int]]), fixed_spans (List[List[int]])\n",
    "    \"\"\"\n",
    "    sentences, original_spans, fixed_spans = [], [], []\n",
    "    data = pd.read_csv(os.path.join(data_path, split + '.csv'))\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        if split == 'tsd_test':\n",
    "            span = fixed_span = []\n",
    "        else:\n",
    "            span = literal_eval(data['spans'][i])\n",
    "            fixed_span = _fix_spans(span, data['text'][i])\n",
    "        \n",
    "        sentences.append(data['text'][i])\n",
    "        original_spans.append(span)\n",
    "        fixed_spans.append(fixed_span)\n",
    "\n",
    "    return sentences, original_spans, fixed_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe073833",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/tsd_trial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be4abb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, original_spans, fixed_spans = get_sentences_from_data_split(\"data\", \"tsd_trial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee1d13",
   "metadata": {},
   "source": [
    "## Tokenize and offset label\n",
    "- BertTokenizer will return the subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "392b82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(self, sentences, spans):\n",
    "    all_token_ids = []\n",
    "    all_offsets = []\n",
    "    all_att_masks = []\n",
    "    all_special_masks = []\n",
    "    all_label_ids = []\n",
    "\n",
    "    for sentence, span in zip(sentences, spans):\n",
    "        # Pad to 512. All sentences in the dataset have a lower number of tokens.\n",
    "        tokenized = self.tokenizer(sentence, padding='max_length', max_length=512, return_attention_mask=True,\n",
    "                                   return_special_tokens_mask=True,\n",
    "                                   return_offsets_mapping=True, return_token_type_ids=False)\n",
    "\n",
    "        all_token_ids.append(tokenized['input_ids'])\n",
    "        all_offsets.append(tokenized['offset_mapping'])\n",
    "        all_att_masks.append(tokenized['attention_mask'])\n",
    "        all_special_masks.append(tokenized['special_tokens_mask'])\n",
    "        all_label_ids.append([self.off2tox(offset, span) for offset in tokenized['offset_mapping']])\n",
    "\n",
    "    return all_token_ids, all_offsets, all_att_masks, all_special_masks, all_label_ids\n",
    "\n",
    "\n",
    "def off2tox(offsets, spans):\n",
    "    # Padded items\n",
    "    if offsets == (0, 0):\n",
    "        return 0\n",
    "    toxicity = offsets[0] in spans\n",
    "    return int(toxicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4e9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenized = tokenizer(sentences[0], return_attention_mask=True,\n",
    "                                   return_special_tokens_mask=True,\n",
    "                                   return_offsets_mapping=True, return_token_type_ids=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eb968dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Because he's a moron and a bigot. It's not any more complicated than that.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef2c36c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 16, 17, 18, 19, 27, 28, 29, 30, 31]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_spans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a9ed241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moron bigot\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0][15:20], sentences[0][27:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d2e2d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'Because ', \"he'\", \"'s\", 's ', 'a ', 'moro', 'on ', 'and ', 'a ', 'bigo', 'ot.', '. ', \"It'\", \"'s\", 's ', 'not ', 'any ', 'more ', 'complicated ', 'than ', 'that.', '.', 'B']\n"
     ]
    }
   ],
   "source": [
    "print([sentences[0][i:j+1] for (i,j) in tokenized[\"offset_mapping\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89db5654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[off2tox(offset, fixed_spans[0]) for offset in tokenized['offset_mapping']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547635e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5038d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca0c62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52d6289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicDataset(Dataset):\n",
    "    def __init__(self, data_path, split):\n",
    "        self.tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        self.original_sentences, self.original_spans, self.fixed_spans = \\\n",
    "            self.get_sentences_from_data_split(data_path, split)\n",
    "\n",
    "        self.token_ids, self.offsets, self.att_masks, self.special_masks, self.labels_ids = \\\n",
    "            self.preprocess_and_tokenize(self.original_sentences, self.fixed_spans)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.original_sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        token_ids = self.token_ids[index]\n",
    "        offsets = self.offsets[index]\n",
    "        att_masks = self.att_masks[index]\n",
    "        special_masks = self.special_masks[index]\n",
    "        label_ids = self.labels_ids[index]\n",
    "        original_spans = self.original_spans[index]\n",
    "        # Add padding to original_spans, which is the only one that is not padded yet.\n",
    "        # All span sets are shorter than 1024\n",
    "        original_spans.extend([-1] * (1024 - len(original_spans)))\n",
    "\n",
    "        # To Tensor\n",
    "        token_ids = torch.tensor(token_ids, dtype=torch.long)\n",
    "        offsets = torch.tensor(offsets, dtype=torch.long)\n",
    "        att_masks = torch.tensor(att_masks, dtype=torch.long)\n",
    "        special_masks = torch.tensor(special_masks, dtype=torch.long)\n",
    "        label_ids = torch.tensor(label_ids, dtype=torch.long)\n",
    "        original_spans = torch.tensor(original_spans, dtype=torch.long)\n",
    "\n",
    "        return token_ids, att_masks, label_ids, offsets, original_spans, special_masks\n",
    "\n",
    "    @staticmethod\n",
    "    def get_sentences_from_data_split(data_path, split):\n",
    "        sentences = []\n",
    "        original_spans = []\n",
    "        fixed_spans = []\n",
    "        data = pd.read_csv(os.path.join(data_path, split + '.csv'))\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            if split == 'tsd_test':\n",
    "                span = fixed_span = []\n",
    "            else:\n",
    "                span = literal_eval(data['spans'][i])\n",
    "                fixed_span = _fix_spans(span, data['text'][i])\n",
    "            sentences.append(data['text'][i])\n",
    "            original_spans.append(span)\n",
    "            fixed_spans.append(fixed_span)\n",
    "\n",
    "        return sentences, original_spans, fixed_spans\n",
    "\n",
    "    def preprocess_and_tokenize(self, sentences, spans):\n",
    "        all_token_ids = []\n",
    "        all_offsets = []\n",
    "        all_att_masks = []\n",
    "        all_special_masks = []\n",
    "        all_label_ids = []\n",
    "\n",
    "        for sentence, span in zip(sentences, spans):\n",
    "            # Pad to 512. All sentences in the dataset have a lower number of tokens.\n",
    "            tokenized = self.tokenizer(sentence, padding='max_length', max_length=512, return_attention_mask=True,\n",
    "                                       return_special_tokens_mask=True,\n",
    "                                       return_offsets_mapping=True, return_token_type_ids=False)\n",
    "\n",
    "            all_token_ids.append(tokenized['input_ids'])\n",
    "            all_offsets.append(tokenized['offset_mapping'])\n",
    "            all_att_masks.append(tokenized['attention_mask'])\n",
    "            all_special_masks.append(tokenized['special_tokens_mask'])\n",
    "            all_label_ids.append([self.off2tox(offset, span) for offset in tokenized['offset_mapping']])\n",
    "\n",
    "        return all_token_ids, all_offsets, all_att_masks, all_special_masks, all_label_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def off2tox(offsets, spans):\n",
    "        # Padded items\n",
    "        if offsets == (0, 0):\n",
    "            return 0\n",
    "        toxicity = offsets[0] in spans\n",
    "        return int(toxicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6626ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.10",
   "language": "python",
   "name": "pytorch1.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
